{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 决策树和k近邻都可以用来分类，但是k近邻无法给出数据的内在含义，而决策树可以\n",
    "# 决策树可能产生过度匹配的问题\n",
    "# 我们构造决策树，首先要找到决定性作用的特征\n",
    "# 那么就需要评估每个特征\n",
    "# 就是不同的往下划分数据子集，直到所有有相同类型的数据都在一个子集中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据可以二分法，也可以其他的什么分成四块这样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集的最大原则就是让无序的数据变得更加有序，也就是信息增益\n",
    "# 熵就是信息的期望值\n",
    "# 待分类的事务可能划分在多个分类中，分类x的信息定义：-log2乘上x的概率\n",
    "# 香农熵：就是计算所有类别所有可能包含的信息期望值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)  # 计算数据集中的实例总数\n",
    "    labelCounts = {}\n",
    "    # 以下五行为所有可能分类创建字典\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]  #提取最后一项做为标签\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            # 如果当前键值不存在，则扩展字典并将当前键值加入字典\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1  # 书中有错\n",
    "    # 0:{\"yes\":1} 1:{\"yes\":2}  2:{\"no\":1} 3:{\"no\":2} 4:{\"no\":3}\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries  # 计算概率\n",
    "        # 以2为底求对数\n",
    "        shannonEnt -= prob * log(prob,2) # 递减求和得熵\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写一个数据集，判断是不是鱼\n",
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    # 不浮出水面是否可以生存，是否有脚蹼\n",
    "    return dataSet,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData, labels = createDataSet()\n",
    "calcShannonEnt(myData)\n",
    "# 熵越高，则混合的数据也越多，可以试试在数据集中添加更多的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3709505944546687"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myData[0][-1] = 'maybe'\n",
    "# 这里我们增加第三个名为maybe的分类，测试熵的变化\n",
    "calcShannonEnt(myData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet,axis,value): # 三个输入参数：待划分的数据集、划分数据集的特征、特征的返回值\n",
    "    # 创建新的list对象\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:  # dataSet[0]=0时执行以下操作\n",
    "            # 以下三行抽取\n",
    "            reducedFeatVec = featVec[:axis]   # featVec[:0]= [],即生成一个空列表\n",
    "            reducedFeatVec.extend(featVec[axis + 1:]) # 添加index==1及后的元素 : 0/1/2 跳过,3:1,4:1\n",
    "            retDataSet.append(reducedFeatVec) #整体作为元素添加 3:[[1,\"no\"]] , 4:[[1,\"no\"],[1,\"no\"]]\n",
    "    return retDataSet\n",
    "# 这个函数是拿来划分数据集的，来判断是否正确的划分了数据集，判断按照哪个特征划分数据集是最好的划分方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'maybe'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myData, 0 ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'no'], [1, 'no']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataSet(myData, 0 ,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'no'], [1, 'no']]\n[[1, 'maybe'], [1, 'yes'], [0, 'no']]\n当i=0时得到的熵为 0.9509775004326936\n[[1, 'no']]\n[[1, 'maybe'], [1, 'yes'], [0, 'no'], [0, 'no']]\n当i=1时得到的熵为 1.2000000000000002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来我们遍历整个数据集，循环计算\n",
    "\n",
    "# 选择最好的数据集划分方式\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1 # 去掉标签项\n",
    "    baseEntropy = calcShannonEnt(dataSet) # 计算熵\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        # 以下两行创建唯一的分类标签列表\n",
    "        featList = [example[i] for example in dataSet] # i=0:[1,1,1,0,0]  i=1:[1,1,0,1,1]\n",
    "        uniqueVals = set(featList)  # i=0:{0,1}  i=1:{0,1}\n",
    "        newEntropy = 0.0\n",
    "        # 以下五行计算每种划分方式的信息熵\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            print(subDataSet)\n",
    "            # i=0:{(0,0),(0,1)} 返回:[[1, 'no'], [1, 'no']]      [[1,\"yes\"],[1,\"yes\"],[0,\"no\"]]\n",
    "            # i=1:{(1,0),(1,1)} 返回:[[0,\"no\"]]       [[1,\"yes\"],[1,\"yes\"],[1,\"no\"],[1,\"no\"]]\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            # i=0:{(0,0),(0,1)} 返回:2/5 3/5\n",
    "            # i=1:{(1,0),(1,1)} 返回:1/5 4/5\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)  # 注意这里是subDataSet 不是 dataSet\n",
    "        print(\"当i={}时得到的熵为\".format(i),newEntropy)\n",
    "        \n",
    "        infoGain = baseEntropy - newEntropy # 信息增益\n",
    "        if (infoGain > bestInfoGain):\n",
    "            # 计算最好的信息增益\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "chooseBestFeatureToSplit(myData)\n",
    "# 熵越小越好，越不混乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到原始的数据集，基于最好的属性划分数据集，递归一次次划分数据\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
